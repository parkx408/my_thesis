\section{Data Deduplication}

The workload presented in typical deduplication systems paper are either from a privately accessible systems~\cite{clements:2009, rhea:2008, manber:1994, aronovich:2009, lillibridge:2009, zhu:2008} or a relatively small dataset of specific type in public domain~\cite{storer:2008, kruus:2010, eshghi:2005}. 
However, either techniques allow a direct comparison of different systems if the dataset under evaluation is completely different or if it only covers a dataset of a particular characteristics.

There have been few work in trying to statistically characterize the input dataset to various benchmark programs~\cite{eeckhout:2003, yi:2002, hsu:2002}. 
However, these papers concentrate on generic sub-setting of dataset so that different characteristics of application is explored statistically hence lacking in accuracy when it comes to predicting exact performance variances.

Numerous works have also been applied in filesystem activities and its contents~\cite{douceur:1999, leung:2008, douceur:1999} with one paper using a simple statistical analysis to characterize a filesystem impression~\cite{agrawal:2009}. 
While these approaches all reveal some interesting intuitions of how we use storage systems, the scope is too generic to be neither accurate or simple enough to be applied in real systems. 
More specific workload analysis have been targeted to SSDs~\cite{soundararajan:2010} and we use some of their approach in out analysis.
