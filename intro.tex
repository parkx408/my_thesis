\chapter{Introduction}
\label{INTRO}
% background
% Storage Cache
% Deduplication System
% Virtualized Storage 

% tools
% ANOVA
% Linear Regression
% Component Analysis
% PB method
% Simulated Anealing

% main body
% 1. statistically accurate ways to compare the performance of two different storage systems.
% 1a. usefulness of the synthetic workload.
% 2. backbox model of storage system that accurately reflects the system's response to different workloads.
% 2a. greybox model of backup workloads that reflects weekly content change. 
% 2b. spatial locality model for storage level cache.  
% 3. application of such model in VM IO load balancing. 
% 4. application of such model in Backup Application.
% 5. application of such model in Storage level cache. 

The current storage systems are designed based on a set of specific workloads. 
It is assumed that these workloads represents the real workload space.
Therefore, the optimizations are typically static and designed for a small number of workloads.
Furthermore, there is no requirement for real time monitoring of worklaods since it is assumed that the entire workload space is already captured. 
Finally, the performance of two storage systems are compared only within those small set of test vectors and the result is deemed final. 

\nohhyun{Potentially talk about the state of academia and cite few papers as exmaple.} 

The goal of this dissertation is to provide means to quantify workload characteristics such that the system can measure workload.
This allows systems to be designed to dynamically adapt to wider spectrum of workloads.
Additionally, it allows system comparison to be based on their response characteristics to different workloads rather than a few predefined workloads. 

\nohhyun{The goal needs to be generic to support the results.}

\begin{figure}[h]
\centering
\input{figure/storage_attributes}
\caption{Storage system operation and attributes.}
\label{fig:storageAttribute}
\end{figure}

A storage system is a system that supports two operations, \emph{load} and \emph{store}.
The \emph{load} operation allows system to change its internal state and the \emph{store} operation allows this state to be verified.
A storage system must further ensure that the interval state is immutable in an absence of a \emph{store} operation.

There are numerous attributes of storage systems which are of our interest.
The first type of attributes, \emph{flexibility} attribute, describe the allowed state transitions.
A generic storage system allows all states to be reached from every other state and will be the focus of this work.
The second type of attribute, \emph{capacity} attribute is the number of the states and defines the storage \emph{capacity}.
The third type of attribute, \emph{power} attribute, defines power requirement for maintaining the states, state transition and state verification.
The fourth type of attribute, \emph{reliability} defines the probability of state transition in the absence of \emph{store} operation.
The last attribute, \emph{performance} attribute, defines speed at which the state transitions occur as well as verification of the states.
These classification of attributes as well as the two primary operations are shown in Figure \ref{fig:storageAttribute}.

The performance evaluation of storage systems is a difficult task due to lack of representative workloads. Storage benchmark tools test a single aspect such as random read performance at a time. While this approach does allow us to gain useful insights to the system behavior, current practices are largely inadequate due to large benchmark input parameter space. For example, a benchmark with 10 parameters require at least 1024 experiments if conducted exhaustively even if we only test extreme values of each parameter. Furthermore, there is no way to tell if the benchmark being used is enough to test all realistic scenarios. As a result, researchers and developers rely on multiple benchmarks with ad hoc input parameters.

We propose a method to quickly identify input parameters that have high effect on the performance metric of interest. We also show that using multiple benchmarks is unnecessary at times and a good benchmark can cover all operational space by providing a control over key parameters that affect the performance metric being measured. 

%%%%%%%%%
%%%%%%%%%

Workload consolidation is a key technique in reducing costs in virtualized datacenters.
When considering storage consolidation, a key problem is the unpredictable performance behavior of consolidated workloads on a given storage system.
In practice, this often forces system administrators to grossly overprovision storage to meet application demands.
In this paper, we show that existing modeling techniques are inaccurate and ineffective in the face of heterogenous devices.
We introduce {\em\romano}, a storage performance management system designed to optimize truly heterogeneous virtualized datacenters.
At its core, \romano constructs and adapts approximate workload-specific performance models of storage devices automatically, along with prediction intervals.
It then applies these models to allow highly efficient IO load balancing.

End-to-end experiments demonstrate that \romano reduces prediction error by 80\% on average compared with existing techniques.
The result is improved load balancing with lowered variance by 82\% and reduced average and maximum latency observed across the storage systems by 52\% and 78\%, respectively.

%%%%%%%%%
%%%%%%%%%

The compression and throughput performance of data deduplication system is directly affected by the input dataset. 
We propose two sets of evaluation metrics, and the means to extract those metrics, for deduplication systems. 
he First set of metrics represents how the composition of segments changes within the deduplication system over five full backups. 
his in turn allows more insights into how the compression ratio will change as data accumulate.
The second set of metrics represents index table fragmentation caused by duplicate elimination and the arrival rate at the underlying storage system.
We show that, while shorter sequences of unique data may be bad for index caching, they provide a more uniform arrival rate which improves the overall throughput.
Finally, we compute the metrics derived from the datasets  under evaluation and show how the datasets perform with different metrics.Our evaluation shows that backup datasets typically exhibit patterns in how they change over time and that these patterns are quantifiable in terms of how they affect the deduplication process.
This quantification allows us to: 1) decide whether deduplication is applicable, 2) provision resources, 3) tune the data deduplication parameters and 4) potentially decide which portion of the dataset is best suited for deduplication.


\begin{itemize}

\item \CHP~\ref{INTRO} introduces the analytic goals pursued in this thesis.

\item \CHP~\ref{BG} prvides a brief overview of the storage systems that are used as part of the experimental platfrom.

\item \CHP~\ref{TOOL} give highlevel description of the statistical tools used to dentify and quantify various components of the workload and to describe their interations with the system. 

\item \CHP~\ref{BENCH} shows means to efficiently compare different benchmark programs ans suggest how they should be used to evaluate storage system performance. 

\item \CHP~\ref{SPM} describes a method to model storage system performance as a function of workloads characteristics in a accurate and efficient way.

\item \CHP~\ref{ROMANO} presents Romano, a load balancing system designed on top of storage performance model constructed in \CHP~\ref{SPM}

\item \CHP~\ref{BW} shows the workload model can effect both the capacity and performance of backup storage system. 

\item \CHP~\ref{RELATED} lists some of the related works in the area.

\item \CHP~\ref{CONCLUSION} concludes the thesis work with discussion of some interesting future works. 
 
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
